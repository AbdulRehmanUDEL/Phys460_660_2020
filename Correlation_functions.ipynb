{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlations, power spectra (and my favorite mathematical physics joke)\n",
    "\n",
    "Consider a pair of time dependent signals or stochastic (random) processes like the two shown in the figure below. They could be fluctuations in the current in a circuit, or the velocity or position a small (Brownian) particle subject to thermal fluctuations, or the deflection of your ear drum as you listen to this lecture, or... As you can see, this is a problem that arises in every corner of science and engineering. In fact, these two tracels are the angular deflection of a *tiny* mirror suspended ffrom a very thin wire. (**Discuss**)\n",
    "\n",
    "![stochastic signals](./two_signals.png)\n",
    "\n",
    "Are these signals random? Partially random and partially predictable? How do we know? What sort of mathematical analysis can answer this question?\n",
    "\n",
    "The answer is provided by a *correlation function.* Let the signal of interest be denoted $x$. The time-correlation (or \"auto-correlation\") is defined as:\n",
    "\n",
    "$$C_x(t,t^\\prime) = \\langle x(t)x(t^\\prime)\\rangle$$\n",
    "\n",
    "The angle brackets mean \"average,\" which is required when we are dealing with observables that have both randomness and some inherent structure. (**Why?**) The average could be a\n",
    "* Ensemble average: Repeat the measurement (or simulation) many times, and then average over multiple realizations\n",
    "* Time average: Collect data for a long time, and treat different $(t,t^{\\prime})$ pairs as independent. \n",
    "\n",
    "**Time translation invariance** Here, we will consider (usually?) processes which are *ergodic.* **Question:** What does ergodic mean? \n",
    "\n",
    "* When a signal is invariant under translation in time, $C_x(t,t^\\prime)$ can only depend on the difference $(t-t^\\prime)$, which is often referred to as the \"lag time\". We can also pick $t^\\prime = 0$, and write $C_x(t) = \\langle x(t)x(0)\\rangle$.\n",
    "* It is common to subtract off the average value of $x$ if it is not zero:\n",
    "\n",
    "$$\\begin{align}\n",
    "C_x(t) & = \\langle \\left( x(t) - \\langle x\\rangle\\right) \\left( x(0) - \\langle x\\rangle\\right)\\rangle\\\\\n",
    " & = \\langle x(t)x(0) \\rangle - \\langle x \\rangle^2\n",
    "\\end{align}$$\n",
    "\n",
    "You can think of this correlation function (when we are in discretized time, as we always are in the computer) as a *covariance matrix*, constrained by symmetry across the diagonal, positivity of the eigenvalues, and time-translation invariance. \n",
    "\n",
    "The time correlation function contains information about the \"memory\" of a signal, that is, how likely it is to return some time later to the same value. Often, given a signal of interest (or some data from a simulation), we might try and compute $C(t)$ directly in the time domain, and then see if it fits some simple model, like an exponential decay. In that case, the time constant of the exponential decay tells you how long it takes, on average, for the signal to \"forget\" where it was at an earlier time. \n",
    "\n",
    "**Question:** How do you expect a completely random signal to be correlated with itself? What about a periodic function like a sine wave?\n",
    "\n",
    "## Fourier transform and the power spectrum\n",
    "It is often more insightful to work in the frequency domain, so let's take a look at the Fourier transform of $C(t)$. Defining our forward and reverse transforms:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\tilde{x}(\\omega) & = \\int_{-\\infty}^{\\infty} dte^{i\\omega t}x(t) \\\\\n",
    "x(t)  & = \\int_{-\\infty}^{\\infty} \\frac{d\\omega}{2\\pi}e^{-i\\omega t}\\tilde{x}(\\omega)\n",
    "\\end{align}$$\n",
    "\n",
    "Now let's compute the correlation function in the frequency domain $\\langle x(\\omega)x(\\omega^\\prime)\\rangle$, and then take advantage of time translation invariance:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\langle \\tilde{x}(\\omega) \\tilde{x}(\\omega^\\prime) \\rangle & = \n",
    "\\langle \\int_{-\\infty}^{\\infty} dt e^{i\\omega t}x(t)  \n",
    "\\int_{-\\infty}^{\\infty} dt^\\prime e^{i\\omega^\\prime t^\\prime}x(t^\\prime)\\rangle \\\\\n",
    "& =  \\int_{-\\infty}^{\\infty} dt e^{i\\omega t}  \n",
    "\\int_{-\\infty}^{\\infty} dt^\\prime e^{i\\omega^\\prime t^\\prime} \\langle x(t) x(t^\\prime)\\rangle \\\\ \n",
    "& = \\int_{-\\infty}^{\\infty} dt e^{i\\omega t} \\int_{-\\infty}^{\\infty} dt^\\prime e^{i\\omega^\\prime t^\\prime} \\int_{-\\infty}^{\\infty} \\frac{d\\Omega}{2\\pi}e^{-i\\Omega(t-t^\\prime)}S_x(\\Omega) \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "where we have introduced the Fourier transform of the correlation function and used invariance under translation in time, so that $C_x$ only depends on the time difference $t-t^\\prime$ which we call $\\tau$:\n",
    "\n",
    "$$\\begin{align}\n",
    "S_x(\\Omega) & = \\int_{-\\infty}^{\\infty}d\\tau e^{i\\Omega\\tau}C_x(\\tau) \\\\\n",
    "C_x(t-t^\\prime) & = \\int_{-\\infty}^{\\infty} \\frac{d\\Omega}{2\\pi}e^{-i\\Omega(t-t^\\prime)}S_x(\\Omega)\n",
    "\\end{align}$$\n",
    "\n",
    "Now let's reorder the integrals to do the time integrals first, then the $\\Omega$ integral:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\langle \\tilde{x}(\\omega) \\tilde{x}(\\omega^\\prime) \\rangle & = \n",
    "\\int_{-\\infty}^{\\infty} \\frac{d\\Omega}{2\\pi}S_x(\\Omega) \\left[ \\int_{-\\infty}^{\\infty} dt e^{i(\\omega-\\Omega)t} \\right] \\left[ \\int_{-\\infty}^{\\infty} dt^\\prime e^{i(\\omega^\\prime+\\Omega)t^\\prime} \\right]\n",
    "\\end{align}$$\n",
    "\n",
    "Recalling the Fourier representation of the Dirac delta function, we recognize in the $[...]$\n",
    "\n",
    "$$\\begin{align}\n",
    "\\int_{-\\infty}^{\\infty} dt e^{i(\\omega-\\Omega)t} & = 2\\pi\\delta(\\omega-\\Omega) \\\\\n",
    "\\int_{-\\infty}^{\\infty} dt^\\prime e^{i(\\omega^\\prime+\\Omega)t^\\prime} & = 2\\pi\\delta(\\omega^\\prime + \\Omega)\n",
    "\\end{align}$$\n",
    "\n",
    "Substituting these back into the previous line we find that the positive and negative frequency components are not independent:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\langle \\tilde{x}(\\omega) \\tilde{x}(\\omega^\\prime) \\rangle & = \n",
    "\\int_{-\\infty}^{\\infty} \\frac{d\\Omega}{2\\pi}S_x(\\Omega) 2\\pi\\delta(\\omega-\\Omega) 2\\pi\\delta(\\omega^\\prime + \\Omega)\\\\\n",
    "  & = 2\\pi\\delta(\\omega^\\prime + \\omega)S_x(\\omega)\n",
    "\\end{align}$$\n",
    "\n",
    "This means that while the signal at different times may be correlated in complicated ways, in the frequency domain $\\tilde{x}(\\omega)$ only covaries with $\\tilde{x}(-\\omega)$. This structure arises because of time translation invariance, and because we are using a complex representation for real variables, which demands that $\\tilde{x}(-\\omega) = \\tilde{x}^*(\\omega)$. \n",
    "\n",
    "The function $S_x(\\omega)$ is called the \"power spectrum\" or \"spectral density,\" and it plays a central role in physics and engineering. It tells us that when we decompose a signal into its frequency components via Fourier transform, those components enter each with a different amplitude. Since in many problems the squared amplitude *is* the power carried by the signal (think back to AC circuits), we call this a \"power spectrum.\" \n",
    "\n",
    "Let's take a look at a few practical examples of signals, their correlation functions, and power spectra. Let's start with a simple periodic function, a sine wave. We can add some noise to it, and see what that does to the autocorrelation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#import math as math\n",
    "import numpy as np\n",
    "import random as rand\n",
    "\n",
    "## I'm going to write this code in a silly way, in order to make the math more obvious\n",
    "## There are *MUCH* smarter/faster ways to compute correlation functions...those will come later\n",
    "\n",
    "## This cell generates a simple sine wave, and superimposes some random noise on top, \n",
    "## and computes the autocorrelation of that signal\n",
    "\n",
    "dt = 0.1\n",
    "N = 100\n",
    "omega = 1.0\n",
    "##\n",
    "## periodic holds a simple sine wave (or superpositions)\n",
    "## noisy_periodic holds the same plus some noise\n",
    "periodic = []\n",
    "noisy_periodic = []\n",
    "time = []\n",
    "\n",
    "for i in range(0,N):\n",
    "    t = i*dt\n",
    "    ##\n",
    "    ## Start with the ordinary sine wave. then use the line above to add noise.\n",
    "    ## Since we will use these same timseries and corr funcs later in the FFT section\n",
    "    ## I'll give them unique names. \n",
    "    \n",
    "    ##NOTE: here is our first use of a pseudo random number, which calls the \"random\" \n",
    "    ## function from the python lib \"random.\" This routine is based on the Mersenne twister\n",
    "    tmp2 = np.sin(omega*t) + np.sin(4.0*omega*t) + 2.0*(rand.random() - 0.5)\n",
    "    #tmp = np.sin(omega*t)\n",
    "    tmp1 = np.sin(omega*t) + np.sin(4.0*omega*t)\n",
    "    periodic.append(tmp1)\n",
    "    noisy_periodic.append(tmp2)\n",
    "    time.append(t)\n",
    "\n",
    "## this list will hold the autocorrelation\n",
    "Ct_periodic = []\n",
    "Ct_noisy_periodic = []\n",
    "\n",
    "# delta is the \"lag time\"\n",
    "for delta in range (0,N-10):\n",
    "    ## total keeps the sum for each time lag, \n",
    "    ## incr counts how many terms contribute to the sum\n",
    "    total = 0\n",
    "    total2 = 0\n",
    "    incr = 0\n",
    "    for i in range (1,N-delta):\n",
    "        incr = incr + 1\n",
    "        # the product at each pair of data lagged by delta t\n",
    "        total = total + periodic[i]*periodic[i+delta]\n",
    "        total2 = total2 + noisy_periodic[i]*noisy_periodic[i+delta]\n",
    "    \n",
    "    ## here we compute the average (this is dumb when the sum has lots of terms)\n",
    "    avg = total/incr\n",
    "    avg2 = total2/incr\n",
    "    Ct_periodic.append(avg)\n",
    "    Ct_noisy_periodic.append(avg2)\n",
    "\n",
    "# cant figure out how to scale the axes of the C(t) plot\n",
    "M_C = len(Ct_periodic)\n",
    "x_axis = np.linspace(0,M_C,M_C)\n",
    "\n",
    "\n",
    "plt.figure(1, figsize=(10, 3))\n",
    "plt.subplots_adjust(wspace=0.5)\n",
    "ax = plt.subplot(131)\n",
    "ax.set_xlabel('time')\n",
    "ax.set_ylabel('signal')\n",
    "plt.plot(time,periodic)\n",
    "plt.plot(time,noisy_periodic)\n",
    "\n",
    "ax2 = plt.subplot(132)\n",
    "ax2.set_xlabel('time x 10')\n",
    "ax2.set_ylabel('C(t)')\n",
    "plt.plot(x_axis,Ct_periodic)\n",
    "plt.plot(x_axis,Ct_noisy_periodic)\n",
    "\n",
    "ax3 = plt.subplot(133)\n",
    "ax3.set_xlabel('time')\n",
    "ax3.set_ylabel('signal')\n",
    "plt.plot(time,periodic)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that a periodic function is perfectly correlated with itself, when the lag time exactly matches the period (or an integer multiple thereof). It is *anticorrelated* at 1/2 the period. Adding some noise degrades the correlation, but the correlation is detectable so long as the signal rises above the noise.\n",
    "\n",
    "**Discussion (important!):** Notice the number of operations required to compute $C_x(t)$, and think about what happens if you have a very long timeseries, and just naively compute the whole damn thing by brute force!\n",
    "\n",
    "Now let's take a look at a more random signal, but one which still has structure. First we will generate a sequence of *independent* Gaussian distributed values, and compute the autocorrelation of this \"signal.\" Since they are independent, there should be no correlation. \n",
    "\n",
    "Then we will generate a sequence of Gaussian distributed numbers, but we will make them correlated, by making each new member of the sequence dependent on the previous one. This algorithm (which I shamelessly stole from Markus Deserno's (CMU Physics) website) looks like\n",
    "\n",
    "$$x_{N+1} = fx_N + \\sqrt{(1-f^2)}g_{N+1}$$\n",
    "\n",
    "where $g_N$ is a random number drawn from a Gaussian distribution of zero mean an unit variance, and $f$ introduce the memory function via a timescale $\\tau$: $f=e^{-1/\\tau}$. So if we simulate such a sequence and compute its correlation function, we expect to get an exponential decay on a timescale of $\\tau$.\n",
    "\n",
    "*Aside:* This type of noise arises often in the context of the motion of a Brownian particle, and is called \"Ornstein-Uhlenbeck\" noise.\n",
    "\n",
    "*Aside2:* Even though most commonly used numerical libraries have built in (pseudo) random number generators, generating sequences of random numbers numerically is not trivial. In fact, most such generators are not really random at all, but deterministic (same seed, same sequence) and repeating (but good generators have very long periods). The random numbers produced by Python's \"random\" library are based on the Mersenne Twister algorithm (Matsumoto and Nishimura, 1998) which has a period of $2^{19937} - 1$. Python's rand documentation contains a warning about using these sequences for cryptographic applications, for obvious reasons.\n",
    "\n",
    "*Aside3:* The Mersenne Twister and its cousins all generate *uniformly* distributed real numbers in the interval [0,1). If you want Gaussian distributed (or Poisson, or Weibull, or...) then you have to apply a transformation to the uniform i.i.d. (\"independent and identically distributed\") numbers. These are already implemented in Python as well. But when I was your age, we coded our own transformations while walking uphill through the snow. On punchcards.\n",
    "\n",
    "Anyway, we will talk a lot more about random numbers later on in the Monte Carlo section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "#import math as math\n",
    "#import numpy as np\n",
    "#import random as rand\n",
    "\n",
    "## Here I use an algorithm that Markus Deserno wrote up in 2002. \n",
    "## It can be found at the CMU biophysics website\n",
    "\n",
    "tau = 4.0\n",
    "f =  np.exp(-1.0/tau)\n",
    "g_scale = np.sqrt(1 - f**2)\n",
    "print(f, g_scale)\n",
    "\n",
    "# for debugging, use the same sequence of rands\n",
    "#rand.seed(2)\n",
    "\n",
    "uni_sig = []\n",
    "corr_sig = []\n",
    "t = []\n",
    "s0 = rand.gauss(0.0, 1.0)\n",
    "uni_sig.append(s0)\n",
    "corr_sig.append(s0)\n",
    "t.append(0)\n",
    "\n",
    "# the loop generates the sequence (\"corr_sig\")of correlated random numbers according to Deserno's algorithm\n",
    "# we will also generate a sequence of Gaussian distributed numbers with no \n",
    "# correlation for comparison (\"uni_sig\")\n",
    "for i in range(0,10000):\n",
    "    tmp2 = f*corr_sig[i-1] + g_scale*rand.gauss(0.0, 1.0)\n",
    "    tmp = rand.gauss(0.0, 1.0)\n",
    "    uni_sig.append(tmp)\n",
    "    corr_sig.append(tmp2)\n",
    "    t.append(i)\n",
    "\n",
    "# calculate the autocorrelation function. \n",
    "# for some reason, every other value of the sequence is anti-correlated. \n",
    "# this must be \"real,\" (ie, not a bug), but it disctracts from the main point \n",
    "# which is to show that the auto correlation captures the exponential decay in the memory\n",
    "\n",
    "uni_Ct = []\n",
    "corr_Ct= []\n",
    "M = len(corr_sig)\n",
    "for delta in range (0,50,2):\n",
    "    total = 0\n",
    "    total2 = 0\n",
    "    incr = 0\n",
    "    for i in range (1,M-delta):\n",
    "        incr = incr + 1\n",
    "        total = total + uni_sig[i]*uni_sig[i+delta]\n",
    "        total2 = total2 + corr_sig[i]*corr_sig[i+delta]\n",
    "        \n",
    "    avg = total/incr\n",
    "    avg2 = total2/incr\n",
    "    uni_Ct.append(avg)\n",
    "    corr_Ct.append(avg2)\n",
    "\n",
    "# normalize the auto correlations by the variance so they start from 1\n",
    "M_C = len(uni_Ct)\n",
    "for i in range(0,M_C):\n",
    "    uni_Ct[i] = uni_Ct[i]/uni_Ct[0]\n",
    "    corr_Ct[i] = corr_Ct[i]/corr_Ct[0]\n",
    "\n",
    "x2_axis = np.linspace(0,M_C,M_C)\n",
    "calc_C = np.exp(-1.0*x2_axis/tau)\n",
    "\n",
    "## start with just the uncorrelated signal and it's autocorr\n",
    "## then add the correlated signal\n",
    "## series2 is the correlated signal\n",
    "plt.figure(1, figsize=(8.5, 4))\n",
    "plt.subplots_adjust(wspace=0.5)\n",
    "ax = plt.subplot(121)\n",
    "ax.set_xlabel('time')\n",
    "ax.set_ylabel('signal')\n",
    "ax.set_xlim(right=100)\n",
    "plt.plot(t,uni_sig)\n",
    "plt.plot(t,corr_sig)\n",
    "\n",
    "\n",
    "ax2 = plt.subplot(122)\n",
    "ax2.set_xlabel('time')\n",
    "ax2.set_ylabel('C(t)')\n",
    "plt.plot(x2_axis,uni_Ct)\n",
    "plt.plot(x2_axis,corr_Ct)\n",
    "plt.plot(x2_axis,calc_C)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The FFT\n",
    "So now we have learned some basics of autocorrelations, and we want to turn to the topic of their Fourier transform (the power spectrum), and learn more about the information that is contained in the power spectrum. That means we need to think about how to compute FT's in the computer. \n",
    "\n",
    "First, we note that in the computer, time is discretized, and therefore frequency is too. It also means that if we want to go from the time to the frequency domain, we are actually computing a discrete Fourier transform (DFT). The same is true for the inverse operation. Therefore all of our integrals above become sums. For example:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\tilde{x}(\\omega_k) & = \\sum_{n=0}^{N-1}x(t_n)e^{-i2\\pi kn/N}  \\hspace{3em} (k=0,1,2,...,N-1) \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "Notice that evaluating this sum directly requires $N^2$ operations: Each of the $N$ different $\\omega$'s requires a sum of $N$ terms.  However, we don't need to evaluate every single term. In fact, the sum is quite *sparse* --- many of the terms are zero. This can be seen by writing the above equation as a matrix multiplication. Let $\\alpha_N = e^{-i2\\pi/N}$: \n",
    "\n",
    "$$\n",
    "\\left(\\begin{array}{c}\n",
    "\\tilde{x}(\\omega_0) \\\\\n",
    "\\tilde{x}(\\omega_1) \\\\\n",
    "\\vdots \\\\\n",
    "\\tilde{x}(\\omega_{N-1})\n",
    "\\end{array}\\right) = \n",
    "\\left(\\begin{array}{cccc} \n",
    "\\alpha_N^{0\\cdot0} & \\alpha_N^{0\\cdot1} &\\cdots & \\alpha_N^{0\\cdot (N-1)}\\\\\n",
    "\\alpha_N^{1\\cdot0} & \\alpha_N^{1\\cdot1} &\\cdots & \\alpha_N^{1\\cdot (N-1)}\\\\\n",
    "\\vdots & \\vdots &\\ddots & \\vdots\\\\\n",
    "\\alpha_N^{(N-1)\\cdot0} & \\alpha_N^{(N-1)\\cdot1} &\\cdots & \\alpha_N^{(N-1)\\cdot (N-1)}\\\\\n",
    "\\end{array}\\right)\n",
    "\\left(\\begin{array}{c} \n",
    "x(t_0) \\\\ \n",
    "x(t_1) \\\\\n",
    "\\vdots \\\\\n",
    "x(t_{N-1})\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "The DFT matrix is highly symmetric, thanks to the properties of $e^{i\\theta}$. Exploiting these symmetries permits a great deal of factorization, reducing the computational complexity to $N\\text{ln}N$. This is called a \"fast Fourier transform\" (FFT), and is a cornerstone of modern numerical methods. Notice that these symmetries are especially evident for vectors with a dimension that is a power of 2, but clever people have worked out tricks for basically any case (I think).\n",
    " \n",
    "We will use the basic FFT algorithm to compute the Fourier transform of $C_x(t)$. First we will take a look at the power spectra for some of the simple cases we looked at first --- a sine wave plus some noise, and also some superpositions of sine waves with different frequencies. Then we will return to our uniform random noise (no time correlations) and the Ornstein-Uhlenbeck noise (exponentially correlated). We will learn why random, uniform noise is called \"white noise,\" and I'll finally get to tell my favorite physics joke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# no one writes their own FFT code these days...but it's not a bad idea \n",
    "# to do once if you will be a serious computational physicist\n",
    "FFT_Ct_period = np.fft.fft(Ct_periodic)\n",
    "FFT_Ct_noisy_period = np.fft.fft(Ct_noisy_periodic)\n",
    "\n",
    "\n",
    "plt.figure(1, figsize=(10, 3))\n",
    "plt.subplots_adjust(wspace=0.5)\n",
    "ax = plt.subplot(131)\n",
    "ax.set_xlabel('time')\n",
    "ax.set_ylabel('signal')\n",
    "plt.plot(time,periodic)\n",
    "plt.plot(time,noisy_periodic)\n",
    "\n",
    "ax2 = plt.subplot(132)\n",
    "ax2.set_xlabel('time x 10')\n",
    "ax2.set_ylabel('C(t)')\n",
    "plt.plot(x_axis,Ct_periodic)\n",
    "plt.plot(x_axis,Ct_noisy_periodic)\n",
    "\n",
    "ax3 = plt.subplot(133)\n",
    "ax3.set_xlabel('$\\omega$')\n",
    "ax3.set_ylabel('$S_x(\\omega)$')\n",
    "ax3.set_xlim(right=25)\n",
    "plt.plot(x_axis,FFT_Ct_period)\n",
    "plt.plot(x_axis,FFT_Ct_noisy_period)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots above show a periodic signal $x(t)$, its autocorrelation $C_x(t)$, and the power spectrum obtained by FFT of $C_x(t)$. Notice that the power spectrum is comprised of \"spikes\" at frequencies corresponding to those in the original signal. (If we had better resolution, they would be sharper, becoming $\\delta-$functions in the continuous limit.) Now we start to see why it is called a \"power spectrum\" --- it tells you which Fourier modes have nonzero amplitudes. \n",
    "\n",
    "Now let's come back to our random signals, and look at their power spectra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing power spectra with FFT\n",
    "\n",
    "# FFT of the original timeseries. \n",
    "FFT_uni_sig = np.fft.fft(uni_sig)\n",
    "FFT_corr_sig = np.fft.fft(corr_sig)\n",
    "\n",
    "\n",
    "# FFT of the autocorrelation functions\n",
    "FFT_uni_Ct = np.fft.fft(uni_Ct)\n",
    "FFT_corr_Ct = np.fft.fft(corr_Ct)\n",
    "\n",
    "\n",
    "## first show the FFT of the uncorrelated random sig and it's power spectrum.\n",
    "## then add the correlated sig\n",
    "\n",
    "plt.figure(1, figsize=(8.5, 8.5))\n",
    "plt.subplots_adjust(wspace=0.5)\n",
    "\n",
    "ax5 = plt.subplot(221)\n",
    "ax5.set_xlabel('time')\n",
    "ax5.set_ylabel('signal')\n",
    "ax5.set_xlim(right=100)\n",
    "plt.plot(t,uni_sig)\n",
    "plt.plot(t,corr_sig)\n",
    "\n",
    "ax6 = plt.subplot(222)\n",
    "ax6.set_xlabel('$\\omega$')\n",
    "ax6.set_ylabel('FT signal')\n",
    "ax6.set_xlim(right=100)\n",
    "plt.plot(t,FFT_uni_sig)\n",
    "plt.plot(t,FFT_corr_sig)\n",
    "\n",
    "ax3 = plt.subplot(223)\n",
    "ax3.set_xlabel('time')\n",
    "ax3.set_ylabel('C(t)')\n",
    "plt.plot(x2_axis,uni_Ct)\n",
    "plt.plot(x2_axis,corr_Ct)\n",
    "#plt.plot(x2_axis,calc_C)\n",
    "\n",
    "ax4 = plt.subplot(224)\n",
    "ax4.set_xlabel('$\\omega$')\n",
    "ax4.set_ylabel('$S_x(\\omega)$')\n",
    "ax4.set_xlim(right=20)\n",
    "plt.plot(x2_axis,FFT_uni_Ct)\n",
    "plt.plot(x2_axis,FFT_corr_Ct)\n",
    "#plt.plot(x2,calc_C)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
